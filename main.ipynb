{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimwa\\anaconda3\\envs\\faqchatbot\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps):\n",
    "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "  train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "  data_collator = load_data_collator(tokenizer)\n",
    "\n",
    "  tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "  model.save_pretrained(output_dir)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "      )\n",
    "\n",
    "  trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "  )\n",
    "      \n",
    "  trainer.train()\n",
    "  trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Ecommerce_FAQ_Chatbot_dataset.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract questions and answers\n",
    "questions = [entry['question'] for entry in data['questions']]\n",
    "answers = [entry['answer'] for entry in data['questions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store questions and answers in a text file\n",
    "with open('train.txt', 'w') as text_file:\n",
    "    for q, a in zip(questions, answers):\n",
    "        text_file.write(f\"[Q] {q}\\n[A] {a}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_file_path = \"/content/drive/MyDrive/ColabNotebooks/data/chatbot_docs/combined_text/full_text/train.txt\"\n",
    "train_file_path = \"train.txt\"\n",
    "model_name = 'gpt2'\n",
    "#output_dir = '/content/drive/MyDrive/ColabNotebooks/models/chat_models/custom_full_text'\n",
    "output_dir = 'custom_q_and_a'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 100\n",
    "save_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimwa\\anaconda3\\envs\\faqchatbot\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 400/400 [02:13<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 133.2579, 'train_samples_per_second': 24.014, 'train_steps_per_second': 3.002, 'train_loss': 0.17795049667358398, 'epoch': 100.0}\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel, GPT2TokenizerFast, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n",
    "def generate_text(model_path, sequence, max_length):\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    ids = tokenizer.encode(f'{sequence}', return_tensors='pt')\n",
    "    final_outputs = model.generate(\n",
    "        ids,\n",
    "        do_sample=True,\n",
    "        max_length=max_length,\n",
    "        pad_token_id=model.config.eos_token_id,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "    )\n",
    "    decoded_text = tokenizer.decode(final_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Split the decoded text by newline character '\\n'\n",
    "    split_text = decoded_text.split('\\n')\n",
    "\n",
    "    # Get the first [Q] and [A] pair\n",
    "    first_qa_pair = split_text[:2]  # Take only the first [Q] and [A]\n",
    "\n",
    "    # Join the [Q] and [A] pair back together with a newline character\n",
    "    formatted_text = '\\n'.join(first_qa_pair)\n",
    "    # print(tokenizer.decode(final_outputs[0], skip_special_tokens=True))\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_path = \"custom_q_and_a\"\n",
    "sequence2 = \"[Q] Do you offer international shipping and how can I track my order? \"\n",
    "max_len = 100\n",
    "answer = generate_text(model2_path,sequence2,max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q] Do you offer international shipping and how can I track my order? [A] If a product is listed as'sold out,' it is currently unavailable for purchase in your country. Please check back later or sign up for notifications when it becomes available again.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL FAQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.35k/3.35k [00:00<00:00, 3.35MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/b-mc2--sql-create-context to C:/Users/kimwa/.cache/huggingface/datasets/b-mc2___json/b-mc2--sql-create-context-21a3552632daf3cf/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.8M/21.8M [00:13<00:00, 1.59MB/s]\n",
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:17<00:00, 17.41s/it]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 105.16it/s]\n",
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to C:/Users/kimwa/.cache/huggingface/datasets/b-mc2___json/b-mc2--sql-create-context-21a3552632daf3cf/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.65it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"b-mc2/sql-create-context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'context', 'answer'],\n",
       "        num_rows: 78577\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_text_generate_question_answer(data, train_name):\n",
    "    # Extract questions and answers\n",
    "    questions = [entry['question'] for entry in data]\n",
    "    answers = [entry['answer'] for entry in data]\n",
    "\n",
    "    # Store questions and answers in a text file\n",
    "    with open(train_name, 'w') as text_file:\n",
    "        for q, a in zip(questions, answers):\n",
    "            text_file.write(f\"[Q] {q}\\n[A] {a}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_generate_question_answer(dataset['train'], 'sql_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = os.path.join(os.getcwd(),\"sql_train.txt\" )\n",
    "model_name = 'gpt2'\n",
    "output_dir = 'custom_sql'\n",
    "overwrite_output_dir = False\n",
    "per_device_train_batch_size = 8\n",
    "num_train_epochs = 20\n",
    "save_steps = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimwa\\anaconda3\\envs\\faqchatbot\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "  0%|          | 56/72800 [00:19<6:39:35,  3.03it/s]"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "# Train\n",
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faqchatbot",
   "language": "python",
   "name": "faqchatbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
